% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/parsnip-neuralprophet.R
\name{neural_prophet}
\alias{neural_prophet}
\title{General Interface for Neural Prophet Time Series Models}
\usage{
neural_prophet(
  mode = "regression",
  growth = NULL,
  user_changepoints = NULL,
  changepoint_num = NULL,
  changepoint_range = NULL,
  seasonality_yearly = NULL,
  seasonality_weekly = NULL,
  seasonality_daily = NULL,
  season = NULL,
  trend_reg = NULL,
  trend_reg_threshold = NULL,
  seasonality_mode = NULL,
  num_hidden_layers = NULL,
  d_hidden = NULL,
  ar_sparsity = NULL,
  learn_rate = NULL,
  epochs = NULL,
  batch_size = NULL,
  loss_func = NULL,
  train_speed = NULL,
  normalize_y = NULL,
  impute_missing = NULL,
  n_forecasts = NULL,
  n_lags = NULL,
  freq = NULL
)
}
\arguments{
\item{mode}{A single character string for the type of model.
The only possible value for this model is "regression".}

\item{growth}{String 'linear' or 'logistic' to specify a linear or logistic trend.}

\item{user_changepoints}{If a list of changepoints is supplied, n_changepoints and changepoints_range are ignored. This list is
instead used to set the dates at which the trend rate is allowed to change.}

\item{changepoint_num}{Number of potential changepoints to include for modeling trend.}

\item{changepoint_range}{Adjusts the flexibility of the trend component by limiting to a percentage of data
before the end of the time series. 0.80 means that a changepoint cannot exist after the first 80\% of the data.}

\item{seasonality_yearly}{One of "auto", TRUE or FALSE. Toggles on/off a seasonal component that
models year-over-year seasonality.}

\item{seasonality_weekly}{One of "auto", TRUE or FALSE. Toggles on/off a seasonal component that
models week-over-week seasonality.}

\item{seasonality_daily}{One of "auto", TRUE or FALSE. Toggles on/off a seasonal componet that
models day-over-day seasonality.}

\item{season}{'additive' (default) or 'multiplicative'.}

\item{trend_reg}{the trend rate changes can be regularized by setting trend_reg to a value greater zero. This is a useful feature that can be used to automatically detect relevant changepoints.}

\item{trend_reg_threshold}{Threshold for the trend regularization}

\item{seasonality_mode}{The default seasonality_mode is additive. This means that no heteroscedasticity is expected in the series in
terms of the seasonality. However, if the series contains clear variance, where the seasonal fluctuations become
larger proportional to the trend, the seasonality_mode can be set to multiplicative.}

\item{num_hidden_layers}{num_hidden_layers defines the number of hidden layers of the FFNNs used in the overall model.
This includes the AR-Net and the FFNN of the lagged regressors. The default is 0, meaning that the FFNNs will have only
one final layer of size n_forecasts. Adding more layers results in increased complexity and also increased computational
time, consequently. However, the added number of hidden layers can help build more complex relationships especially useful
for the lagged regressors. To tradeoff between the computational complexity and the improved accuracy the num_hidden_layers
is recommended to be set in between 1-2. Nevertheless, in most cases a good enough performance can be achieved by having
no hidden layers at all.}

\item{d_hidden}{d_hidden is the number of units in the hidden layers. This is only considered if num_hidden_layers
is specified, otherwise ignored. The default value for d_hidden if not specified is (n_lags + n_forecasts). If tuned
manually, the recommended practice is to set a value in between n_lags and n_forecasts for d_hidden. It is also
important to note that with the current implementation, NeuralProphet sets the same d_hidden for the all the hidden
layers.}

\item{ar_sparsity}{NeuralProphet also contains a number of regularization parameters to control the model coefficients
and introduce sparsity into the model. This also helps avoid overfitting of the model to the training data.For
ar_sparsity values in the range 0-1 are expected with 0 inducing complete sparsity and 1 imposing no regularization at
all. ar_sparsity along with n_lags can be used for data exploration and feature selection. You can use a larger number
of lags thanks to the scalability of AR-Net and use the scarcity to identify important influence of past time steps on
the prediction accuracy.}

\item{learn_rate}{NeuralProphet is fit with stochastic gradient descent - more precisely, with an AdamW optimizer and
a One-Cycle policy. If the parameter learning_rate is not specified, a learning rate range test is conducted to
determine the optimal learning rate. A number for the rate at which the algorithm adapts from iteration-to-iteration.}

\item{epochs}{The epochs and the loss_func are two other parameters that directly affect the model training process.
If not defined, both are automatically set based on the dataset size. They are set in a manner that controls the
total number training steps to be around 1000 to 4000.}

\item{batch_size}{number of samples that will be propagated through the network}

\item{loss_func}{The default loss function is the 'Huber' loss, which is considered to be robust to outliers.
However, you are free to choose the standard MSE or any other PyTorch torch.nn.modules.loss loss function.}

\item{train_speed}{Number indicating the speed at which training of the network occurs.}

\item{normalize_y}{is about scaling the time series before modelling. By default, NeuralProphet performs a (soft)
min-max normalization of the time series. Normalization can help the model training process if the series values
fluctuate heavily. However, if the series does not such scaling, users can turn this off or select another
normalization.}

\item{impute_missing}{is about imputing the missing values in a given series. S imilar to Prophet, NeuralProphet
too can work with missing values when it is in the regression mode without the AR-Net. However, when the
autocorrelation needs to be captured, it is necessary for the missing values to be imputed, since then the
modelling becomes an ordered problem. Letting this parameter at its default can get the job done perfectly in most
cases.}

\item{n_forecasts}{is the size of the forecast horizon. The default value of 1 means that the model forecasts one
step into the future.}

\item{n_lags}{defines whether the AR-Net is enabled (if n_lags > 0) or not. The value for n_lags is usually
recommended to be greater than n_forecasts, if possible since it is preferable for the FFNNs to encounter at
least n_forecasts length of the past in order to predict n_forecasts into the future. Thus, n_lags determine
how far into the past the auto-regressive dependencies should be considered. This could be a value chosen based
on either domain expertise or an empirical analysis.}

\item{freq}{A pandas timeseries frequency such as "5min" for 5-minutes or "D" for daily. Refer to Pandas Offset Aliases}
}
\description{
\code{neural_prophet()} is a way to generate a \emph{specification} of a NEURAL PROPHET model
before fitting and allows the model to be created using
different packages. Currently the only package is \code{neuralprophet} from Python through \code{reticulate}.
}
\details{
The data given to the function are not saved and are only used
to determine the \emph{mode} of the model. For \code{neural_prophet()}, the
mode will always be "regression".

The model can be created using the \code{fit()} function using the
following \emph{engines}:
\itemize{
\item "prophet" (default) - Connects to neuralprophet.NeuralProphet() (Python)
}

\strong{Main Arguments}

The main arguments (tuning parameters) for the \strong{NEURAL PROPHET} model are:
\itemize{
\item \code{growth}: String 'linear' or 'logistic' to specify a linear or logistic trend.
\item \code{changepoint_num}: Number of potential changepoints to include for modeling trend.
\item \code{changepoint_range}: Range changepoints that adjusts how close to the end
the last changepoint can be located.
\item \code{season}: 'additive' (default) or 'multiplicative'.
\item \code{ar_sparsity}: For ar_sparsity values in the range 0-1 are expected with 0 inducing complete sparsity and 1 imposing no regularization at
all
\item \code{num_hidden_layers}: num_hidden_layers defines the number of hidden layers of the FFNNs used in the overall model.
\item \code{d_hidden}: d_hidden is the number of units in the hidden layers.
\item \code{trend_reg}: the trend rate changes can be regularized by setting trend_reg to a value greater zero.
This is a useful feature that can be used to automatically detect relevant changepoints.
}

These arguments are converted to their specific names at the
time that the model is fit.

Other options and argument can be
set using \code{set_engine()} (See Engine Details below).

If parameters need to be modified, \code{update()} can be used
in lieu of recreating the object from scratch.
}
\section{Engine Details}{


The standardized parameter names in \code{neuralprophet} can be mapped to their original
names in each engine.
Other options can be set using \code{set_engine()}.

\strong{prophet}

Limitations:
\itemize{
\item \code{prophet::add_seasonality()} is not currently implemented. It's used to
specify non-standard seasonalities using fourier series. An alternative is to use
\code{step_fourier()} and supply custom seasonalities as Extra Regressors.
\strong{Date and Date-Time Variable}
}

It's a requirement to have a date or date-time variable as a predictor.
The \code{fit()} interface accepts date and date-time features and handles them internally.
\itemize{
\item \code{fit(y ~ date)}
}

\strong{Univariate (No Extra Regressors):}

For univariate analysis, you must include a date or date-time feature. Simply use:
\itemize{
\item Formula Interface (recommended): \code{fit(y ~ date)} will ignore xreg's.
}

\strong{Events}

To include events correctly, the following conditions must be met:
\itemize{
\item Event variable names must contain \code{events} in their name. For example: "events_one", "events_two".
\item Pass a list called \code{add_events} through set_engine(). This list will define the characteristics of our events.
It should contain the elements contained in the \code{add_events} method in Python.
\item Include events in the formula as external regressors
}

\emph{Example:}

neural_prophet(freq = "D") \%>\% set_engine(add_events = list(events = c("events_1", "events_2"),
regularization = 0.5)) \%>\%
fit(y ~ date + events_1 + events_2, data = df)

\strong{Future Regressor}

To include Future Regressors correctly, the following conditions must be met:
\itemize{
\item Future Regressors variable names must contain \code{future_} in their name. For example: "future_one", "future_two".
\item Any columns that aren't labeled "event_" or "lagged_" are added as Future Regressors (except date one).
\item Pass a list called \code{add_future_regressor} through set_engine(). This list will define the characteristics of our future_regressors
It should contain the elements contained in the \code{add_future_regressor} method in Python.
\item Include future_regressors in the formula as external regressors
}

\emph{Example:}

neural_prophet(freq = "D") \%>\% set_engine(add_future_regressor = list(name = c("future_1", "future_2"),
regularization = 0.5)) \%>\%
fit(y ~ date + future_1 + future_2, data = df)

\strong{Lagged Regressor}

To include Lagged Regressors correctly, the following conditions must be met:
\itemize{
\item Lagged Regressors variable names must contain \code{lagged} in their names. For example: "lagged_one", "lagged_two".
\item Pass a list called \code{add_lagged_regressor} through set_engine(). This list will define the characteristics of our lagged_regressor
It should contain the elements contained in the \code{add_lagged_regressor} method in Python.
\item Include lagged regressors in the formula as external regressors
}

\emph{Example:}

neural_prophet(freq = "D") \%>\% set_engine(add_lagged_regressor = list(name = c("lagged_1", "lagged_2"),
regularization = 0.5)) \%>\%
fit(y ~ date + lagged_1 + lagged_2, data = df)
}

\examples{
library(dplyr)
library(lubridate)
library(parsnip)
library(rsample)
library(timetk)

# Data
md10 <- m4_daily \%>\% filter(id == "D10")
md10

# Split Data 80/20
splits <- initial_time_split(md10, prop = 0.8)

# ---- NEURAL PROPHET ----

# Model Spec
model_spec <- neural_prophet(
    freq = "D"
) \%>\%
    set_engine("prophet")

# Fit Spec
model_fit <- model_spec \%>\%
    fit(log(value) ~ date,
        data = training(splits))
model_fit

}
\seealso{
\code{\link[=fit.model_spec]{fit.model_spec()}}, \code{\link[=set_engine]{set_engine()}}
}
